{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Indentures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/kathy/Desktop/Kanerai/example1.pdf'\n",
    "text = convert_pdf_to_txt(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk import sent_tokenize,word_tokenize \n",
    "from nltk.corpus import wordnet as wn\n",
    "import gensim.summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1\n",
    "raw1 = \"\"\"\"Unsalable Asset\" means any Asset with respect to which the Collateral Manager certifies to the Trustee that (a) either (i) such Asset is (A) a Defaulted Obligation, (B) an Equity Security, (C) an obligation received in connection with an offer, in a restructuring or plan of reorganization with respect to the obligor, or (D) any other exchange or any other security or debt obligation that is part of the Assets, in the case of (A), (B) or (C) in respect of which the Issuer has not received a payment in cash during the preceding 12 months or (ii) such Asset has a Market Value of less than $10,000 and (b) the Collateral Manager has made commercially reasonable efforts to dispose of such Asset for at least 90 days and (y) in its commercially reasonable judgment such Asset is not expected to be saleable for the foreseeable future.\n",
    "\"\"\"\n",
    "\n",
    "# example 2\n",
    "raw2 = \"\"\"\"Unsalable Asset\" means any Asset with respect to which the Collateral Manager certifies to the Trustee that (a) either (i) such Asset is (A) a Defaulted Obligation, (B) an Equity Security, (C) an obligation received in connection with an offer, in a restructuring or plan of reorganization with respect to the obligor, or (D) any other exchange or any other security or debt obligation that is part of the Assets, in the case of (A), (B) or (C) in respect of which the Issuer has not received a payment in cash during the preceding 12 months or (ii) such Asset has a Market Value of less than $10,000 and (b) the Collateral Manager has made commercially reasonable efforts to dispose of such Asset for at least 90 days and (y) in its commercially reasonable judgment such Asset is not expected to be saleable for the foreseeable future.\n",
    "\"\"\"\n",
    "\n",
    "# example 1\n",
    "raw3 = \"\"\"\"Adjusted Weighted Average Moody's Rating Factor\": As of any date of determination, a number equal to the Weighted Average Moody's Rating Factor determined in the following manner\": for purposes of determining a Moody's Default Probability Rating or Moody's Rating in connection with determining the Weighted Average Moody's Rating Factor for purposes of this definition, each applicable rating on credit watch by Moody's that is on (a) positive watch will be treated as having been upgraded by one rating subcategory, (b) negative watch will be treated as having been downgraded by two rating subcategories and (c) negative outlook will be treated as having been downgraded by one rating subcategory.\n",
    "\"\"\"\n",
    "\n",
    "# example 1\n",
    "raw4 = \"\"\"\"Target Initial Par Condition\" means a condition satisfied as of the Effective Date if the aggregate principal balance of Collateral Obligations that are held by the Issuer and that the Issuer has committed to purchase on such date, together with the amount of any proceeds of prepayments, maturities or redemptions of Collateral Obligations purchased by the Issuer prior to such date (other than any such proceeds that have been reinvested in Collateral Obligations held by the Issuer on the Effective Date), will equal or exceed the Target Initial Par Amount; provided that for purposes of this definition, any Collateral Obligation that becomes a Defaulted Obligation prior to the Effective Date shall be treated as having a principal balance equal to its Moody's Collateral Value.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprosessing(raw):\n",
    "    wordlist = nltk.word_tokenize(raw)\n",
    "    text = [w.lower() for w in wordlist if w not in stopwords]\n",
    "    return text\n",
    "\n",
    "text1= preprosessing(raw1)\n",
    "text2= preprosessing(raw2)\n",
    "text3= preprosessing(raw3)\n",
    "text4= preprosessing(raw4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-0bec56b75a7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocsim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimilarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlsi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLsiModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvec_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.similarities.docsim import Similarity\n",
    "from gensim import corpora, models, similarities\n",
    "lsi = models.LsiModel(text1, id2word=corpus, num_topics=2)\n",
    "\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "sims = index[vec_lsi]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)\n",
      "[[0.06138911 0.06138911 0.06138911 0.06138911 0.         0.\n",
      "  0.         0.18416732 0.08126564 0.12189846 0.         0.\n",
      "  0.         0.36833464 0.06138911 0.06138911 0.         0.\n",
      "  0.04063282 0.         0.         0.         0.06138911 0.06138911\n",
      "  0.06138911 0.09939955 0.12277821 0.         0.         0.04969977\n",
      "  0.         0.         0.06138911 0.06138911 0.         0.04969977\n",
      "  0.         0.         0.         0.         0.06138911 0.\n",
      "  0.06138911 0.         0.         0.06138911 0.06138911 0.\n",
      "  0.06138911 0.         0.06138911 0.06138911 0.         0.\n",
      "  0.08126564 0.06138911 0.06138911 0.14909932 0.         0.\n",
      "  0.         0.         0.06138911 0.24379692 0.         0.14909932\n",
      "  0.04969977 0.04969977 0.06138911 0.06138911 0.06138911 0.06138911\n",
      "  0.12277821 0.         0.06138911 0.         0.04969977 0.06138911\n",
      "  0.         0.         0.12277821 0.         0.14909932 0.\n",
      "  0.06138911 0.24379692 0.06138911 0.         0.         0.24379692\n",
      "  0.09939955 0.         0.         0.06138911 0.06138911 0.06138911\n",
      "  0.         0.06138911 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.12277821 0.12277821 0.         0.         0.06138911 0.18416732\n",
      "  0.06138911 0.06138911 0.         0.12277821 0.         0.\n",
      "  0.         0.19879909 0.         0.04969977 0.08126564 0.36569539\n",
      "  0.         0.2031641  0.         0.         0.06138911 0.\n",
      "  0.06138911 0.         0.04969977 0.         0.         0.12277821\n",
      "  0.         0.12189846]\n",
      " [0.06138911 0.06138911 0.06138911 0.06138911 0.         0.\n",
      "  0.         0.18416732 0.08126564 0.12189846 0.         0.\n",
      "  0.         0.36833464 0.06138911 0.06138911 0.         0.\n",
      "  0.04063282 0.         0.         0.         0.06138911 0.06138911\n",
      "  0.06138911 0.09939955 0.12277821 0.         0.         0.04969977\n",
      "  0.         0.         0.06138911 0.06138911 0.         0.04969977\n",
      "  0.         0.         0.         0.         0.06138911 0.\n",
      "  0.06138911 0.         0.         0.06138911 0.06138911 0.\n",
      "  0.06138911 0.         0.06138911 0.06138911 0.         0.\n",
      "  0.08126564 0.06138911 0.06138911 0.14909932 0.         0.\n",
      "  0.         0.         0.06138911 0.24379692 0.         0.14909932\n",
      "  0.04969977 0.04969977 0.06138911 0.06138911 0.06138911 0.06138911\n",
      "  0.12277821 0.         0.06138911 0.         0.04969977 0.06138911\n",
      "  0.         0.         0.12277821 0.         0.14909932 0.\n",
      "  0.06138911 0.24379692 0.06138911 0.         0.         0.24379692\n",
      "  0.09939955 0.         0.         0.06138911 0.06138911 0.06138911\n",
      "  0.         0.06138911 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.12277821 0.12277821 0.         0.         0.06138911 0.18416732\n",
      "  0.06138911 0.06138911 0.         0.12277821 0.         0.\n",
      "  0.         0.19879909 0.         0.04969977 0.08126564 0.36569539\n",
      "  0.         0.2031641  0.         0.         0.06138911 0.\n",
      "  0.06138911 0.         0.04969977 0.         0.         0.12277821\n",
      "  0.         0.12189846]\n",
      " [0.         0.         0.         0.         0.06468586 0.\n",
      "  0.         0.         0.03375576 0.03375576 0.06468586 0.\n",
      "  0.2039963  0.         0.         0.         0.19405758 0.\n",
      "  0.10126729 0.         0.15299722 0.2039963  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.04128815\n",
      "  0.06468586 0.05099907 0.         0.         0.06468586 0.\n",
      "  0.05099907 0.06468586 0.06468586 0.12937172 0.         0.12937172\n",
      "  0.         0.06468586 0.         0.         0.         0.05099907\n",
      "  0.         0.         0.         0.         0.19405758 0.06468586\n",
      "  0.06751153 0.         0.         0.         0.         0.15299722\n",
      "  0.         0.         0.         0.06751153 0.         0.04128815\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.06468586 0.         0.         0.         0.\n",
      "  0.30599445 0.12937172 0.         0.06468586 0.         0.\n",
      "  0.         0.13502306 0.         0.10199815 0.12937172 0.03375576\n",
      "  0.         0.06468586 0.         0.         0.         0.\n",
      "  0.06468586 0.         0.         0.         0.         0.06468586\n",
      "  0.         0.         0.         0.         0.10199815 0.58217274\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.06468586\n",
      "  0.12937172 0.         0.         0.         0.03375576 0.10126729\n",
      "  0.05099907 0.03375576 0.         0.15299722 0.         0.06468586\n",
      "  0.         0.06468586 0.         0.19405758 0.19405758 0.\n",
      "  0.15299722 0.03375576]\n",
      " [0.         0.         0.         0.         0.         0.07322125\n",
      "  0.14644249 0.         0.03820988 0.11462965 0.         0.07322125\n",
      "  0.11545694 0.         0.         0.         0.         0.14644249\n",
      "  0.03820988 0.07322125 0.05772847 0.17318542 0.         0.\n",
      "  0.         0.23368087 0.         0.07322125 0.14644249 0.\n",
      "  0.         0.28864236 0.         0.         0.         0.04673617\n",
      "  0.05772847 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.21966374 0.         0.         0.11545694\n",
      "  0.         0.07322125 0.         0.         0.         0.\n",
      "  0.03820988 0.         0.         0.04673617 0.07322125 0.05772847\n",
      "  0.14644249 0.07322125 0.         0.03820988 0.14644249 0.\n",
      "  0.18694469 0.04673617 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.07322125 0.04673617 0.\n",
      "  0.05772847 0.         0.         0.         0.09347235 0.21966374\n",
      "  0.         0.22925931 0.         0.11545694 0.         0.07641977\n",
      "  0.04673617 0.         0.14644249 0.         0.         0.\n",
      "  0.         0.         0.07322125 0.14644249 0.14644249 0.\n",
      "  0.14644249 0.07322125 0.07322125 0.07322125 0.05772847 0.\n",
      "  0.         0.         0.07322125 0.07322125 0.         0.\n",
      "  0.         0.         0.07322125 0.         0.07322125 0.\n",
      "  0.         0.14020852 0.14644249 0.04673617 0.19104942 0.38209884\n",
      "  0.05772847 0.15283954 0.07322125 0.05772847 0.         0.\n",
      "  0.         0.         0.04673617 0.         0.         0.\n",
      "  0.05772847 0.03820988]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [raw1, raw2, raw3, raw4]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "word = vectorizer.get_feature_names()\n",
    "# print (word)\n",
    "# frequency matrix\n",
    "# print (X.toarray())\n",
    " \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "print (transformer)\n",
    "tfidf = transformer.fit_transform(X)\n",
    "# weight(tf-idf) matrix\n",
    "print (tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "input must have more than one sentence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-12f036ef4ee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstriptext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mraw4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstriptext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstriptext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstriptext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/summarization/summarizer.py\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(text, ratio, word_count, split)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;31m# If only one sentence is present, the function raises an error (Avoids ZeroDivisionError).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input must have more than one sentence\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;31m# Warns if the text is too short.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: input must have more than one sentence"
     ]
    }
   ],
   "source": [
    "striptext = (raw4).replace('\\n\\n', ' ')\n",
    "striptext = striptext.replace('\\n', ' ')\n",
    "summary = gensim.summarization.summarize(striptext, word_count=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-1bb58e2f3e31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocsim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimilarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlsi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLsiModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvec_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/lsimodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, chunksize, decay, distributed, onepass, power_iters, extra_samples, dtype)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "from gensim.similarities.docsim import Similarity\n",
    "from gensim import corpora, models, similarities\n",
    "lsi = models.LsiModel(text1, id2word=corpus, num_topics=1)\n",
    "\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "sims = index[vec_lsi]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsindiamag.com/nlp-case-study-identifabsy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "model_path = './word2vec/GoogleNews-vectors-negative300.bin'\n",
    "w2v_model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DocSim(object):\n",
    "    def __init__(self, w2v_model , stopwords=[]):\n",
    "        self.w2v_model = w2v_model\n",
    "        self.stopwords = stopwords\n",
    "\n",
    "    def vectorize(self, doc):\n",
    "        \"\"\"Identify the vector values for each word in the given document\"\"\"\n",
    "        doc = doc.lower()\n",
    "        words = [w for w in doc.split(\" \") if w not in self.stopwords]\n",
    "        word_vecs = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                vec = self.w2v_model[word]\n",
    "                word_vecs.append(vec)\n",
    "            except KeyError:\n",
    "                # Ignore, if the word doesn't exist in the vocabulary\n",
    "#                 print(word)\n",
    "                pass\n",
    "\n",
    "        # Assuming that document vector is the mean of all the word vectors\n",
    "        # PS: There are other & better ways to do it.\n",
    "        vector = np.mean(word_vecs, axis=0)\n",
    "        return vector\n",
    "\n",
    "\n",
    "    def _cosine_sim(self, vecA, vecB):\n",
    "        \"\"\"Find the cosine similarity distance between two vectors.\"\"\"\n",
    "        csim = np.dot(vecA, vecB) / (np.linalg.norm(vecA) * np.linalg.norm(vecB))\n",
    "        if np.isnan(np.sum(csim)):\n",
    "            return 0\n",
    "        return csim\n",
    "\n",
    "    def calculate_similarity(self, source_doc, target_docs=[], threshold=0):\n",
    "        \"\"\"Calculates & returns similarity scores between given source document & all\n",
    "        the target documents.\"\"\"\n",
    "        if isinstance(target_docs, str):\n",
    "            target_docs = [target_docs]\n",
    "\n",
    "        source_vec = self.vectorize(source_doc)\n",
    "        results = []\n",
    "        for doc in target_docs:\n",
    "            target_vec = self.vectorize(doc)\n",
    "            sim_score = self._cosine_sim(source_vec, target_vec)\n",
    "            if sim_score > threshold:\n",
    "                results.append({\n",
    "                    'score' : sim_score,\n",
    "                    'doc' : doc\n",
    "                })\n",
    "            # Sort results by score in desc order\n",
    "            results.sort(key=lambda k : k['score'] , reverse=True)\n",
    "\n",
    "        return results\n",
    "\n",
    "        \n",
    "\n",
    "ds = DocSim(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.87191427, 'doc': 'delete a invoice'}]\n"
     ]
    }
   ],
   "source": [
    "source_doc = 'how to delete an invoice'\n",
    "target_docs = 'delete a invoice'\n",
    "\n",
    "# This will return 3 target docs with similarity score\n",
    "sim_scores = ds.calculate_similarity(source_doc, target_docs)\n",
    "\n",
    "print(sim_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_scores = ds.calculate_similarity(source_doc, target_docs, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Unsalable Asset\" means any Asset with respect to which the Collateral Manager certifies to the Trustee that (a) either (i) such Asset is (A) a Defaulted Obligation, (B) an Equity Security, (C) an obligation received in connection with an offer, in a restructuring or plan of reorganization with respect to the obligor, or (D) any other exchange or any other security or debt obligation that is part of the Assets, in the case of (A), (B) or (C) in respect of which the Issuer has not received a payment in cash during the preceding 12 months or (ii) such Asset has a Market Value of less than $10,000 and (b) the Collateral Manager has made commercially reasonable efforts to dispose of such Asset for at least 90 days and (y) in its commercially reasonable judgment such Asset is not expected to be saleable for the foreseeable future.\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.90400124,\n",
       "  'doc': '\"Target Initial Par Condition\" means a condition satisfied as of the Effective Date if the aggregate principal balance of Collateral Obligations that are held by the Issuer and that the Issuer has committed to purchase on such date, together with the amount of any proceeds of prepayments, maturities or redemptions of Collateral Obligations purchased by the Issuer prior to such date (other than any such proceeds that have been reinvested in Collateral Obligations held by the Issuer on the Effective Date), will equal or exceed the Target Initial Par Amount; provided that for purposes of this definition, any Collateral Obligation that becomes a Defaulted Obligation prior to the Effective Date shall be treated as having a principal balance equal to its Moody\\'s Collateral Value.\\n'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_scores = ds.calculate_similarity(raw1, raw4)\n",
    "sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Target Initial Par Condition\" means a condition satisfied as of the Effective Date if the aggregate principal balance of Collateral Obligations that are held by the Issuer and that the Issuer has committed to purchase on such date, together with the amount of any proceeds of prepayments, maturities or redemptions of Collateral Obligations purchased by the Issuer prior to such date (other than any such proceeds that have been reinvested in Collateral Obligations held by the Issuer on the Effective Date), will equal or exceed the Target Initial Par Amount; provided that for purposes of this definition, any Collateral Obligation that becomes a Defaulted Obligation prior to the Effective Date shall be treated as having a principal balance equal to its Moody\\'s Collateral Value.\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "unpickling stack underflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-d97c7ad04ac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./word2vec/apnews_dbow.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"I opened a new mailbox\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \"\"\"\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \"\"\"\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: unpickling stack underflow"
     ]
    }
   ],
   "source": [
    "import gensim  \n",
    "\n",
    "model = gensim.models.Doc2Vec.load('./word2vec/apnews_dbow.tar')  \n",
    "\n",
    "new_sentence = \"I opened a new mailbox\".split(\" \")  \n",
    "model.docvecs.most_similar(positive=[model.infer_vector(new_sentence)],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyemd import emd\n",
    "from gensim.similarities import WmdSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen: 0.7699\n",
      "queen: 0.8965\n",
      "cereal\n",
      "dog: 0.8798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "result = word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))\n",
    "\n",
    "result = word_vectors.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))\n",
    "\n",
    "print(word_vectors.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
    "\n",
    "similarity = word_vectors.similarity('woman', 'man')\n",
    "\n",
    "result = word_vectors.similar_by_word(\"cat\")\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
    "sentence_president = 'The president greet the press in Chicago'.lower().split()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "sentence_obama = [w for w in sentence_obama if w not in stopwords]\n",
    "sentence_president = [w for w in sentence_president if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['obama', 'speaks', 'media', 'illinois']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_obama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['president', 'greet', 'press', 'chicago']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_president"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1974\n",
      "0.0\n",
      "0.7067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = word_vectors.wmdistance(sentence_obama, sentence_president)\n",
    "print(\"{:.4f}\".format(similarity))\n",
    "\n",
    "distance = word_vectors.distance(\"media\", \"media\")\n",
    "print(\"{:.1f}\".format(distance))\n",
    "\n",
    "sim = word_vectors.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant'])\n",
    "print(\"{:.4f}\".format(sim))\n",
    "\n",
    "vector = word_vectors['computer']  # numpy vector of a word\n",
    "vector.shape\n",
    "\n",
    "vector = word_vectors.wv.word_vec('office', use_norm=True)\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5924055796915986"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = word_vectors.wmdistance(raw1, raw4)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6475005401123706"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = word_vectors.wmdistance('I am happy', 'I am not happy')\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.401189256181643"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = word_vectors.wmdistance('I am happy', 'I am sad')\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7763374853936251"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = word_vectors.wmdistance('it rains', 'it does not rains')\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyemd import emd\n",
    "from gensim.similarities import WmdSimilarity\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import TaggedLineDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kathy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "  (0, 0)\t0.5773502691896258\n",
      "  (0, 2)\t0.5773502691896258\n",
      "  (0, 1)\t0.5773502691896258\n",
      "  (1, 0)\t0.5773502691896258\n",
      "  (1, 2)\t0.5773502691896258\n",
      "  (1, 1)\t0.5773502691896258\n",
      "1.0000000000000002\n",
      "  (0, 0)\t0.5773502691896258\n",
      "  (0, 3)\t0.5773502691896258\n",
      "  (0, 1)\t0.5773502691896258\n",
      "  (1, 0)\t0.44832087319911734\n",
      "  (1, 3)\t0.44832087319911734\n",
      "  (1, 1)\t0.44832087319911734\n",
      "  (1, 2)\t0.6300993445179441\n",
      "0.7765145304745156\n",
      "  (0, 0)\t0.4494364165239821\n",
      "  (0, 5)\t0.6316672017376245\n",
      "  (0, 3)\t0.6316672017376245\n",
      "  (1, 0)\t0.37997836159100784\n",
      "  (1, 2)\t0.534046329052269\n",
      "  (1, 4)\t0.534046329052269\n",
      "  (1, 1)\t0.534046329052269\n",
      "0.17077611319011649\n"
     ]
    }
   ],
   "source": [
    "import nltk, string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt') # if necessary...\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "'''remove punctuation, lowercase, stem'''\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize)\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "#     print(text1, text2)\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    print(tfidf)\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "\n",
    "\n",
    "print (cosine_sim('a little bird', 'a little bird'))\n",
    "print (cosine_sim('a little bird', 'a little bird chirps'))\n",
    "print (cosine_sim('a little bird', 'a big dog barks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'not', 'happi']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_tokens(nltk.word_tokenize('I am not happy'.lower().translate(remove_punctuation_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'not', 'happi', 'haha']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize('I am not happy haha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 71)\t0.04476614810358452\n",
      "  (0, 7)\t0.31336303672509164\n",
      "  (0, 45)\t0.04476614810358452\n",
      "  (0, 6)\t0.13429844431075358\n",
      "  (0, 74)\t0.13429844431075358\n",
      "  (0, 61)\t0.13429844431075358\n",
      "  (0, 69)\t0.22383074051792262\n",
      "  (0, 73)\t0.08953229620716904\n",
      "  (0, 68)\t0.4028953329322607\n",
      "  (0, 15)\t0.08953229620716904\n",
      "  (0, 43)\t0.08953229620716904\n",
      "  (0, 14)\t0.04476614810358452\n",
      "  (0, 70)\t0.04476614810358452\n",
      "  (0, 67)\t0.08953229620716904\n",
      "  (0, 3)\t0.31336303672509164\n",
      "  (0, 25)\t0.04476614810358452\n",
      "  (0, 33)\t0.04476614810358452\n",
      "  (0, 65)\t0.1790645924143381\n",
      "  (0, 36)\t0.13429844431075358\n",
      "  (0, 21)\t0.04476614810358452\n",
      "  (0, 48)\t0.13429844431075358\n",
      "  (0, 9)\t0.13429844431075358\n",
      "  (0, 4)\t0.13429844431075358\n",
      "  (0, 26)\t0.04476614810358452\n",
      "  (0, 64)\t0.08953229620716904\n",
      "  :\t:\n",
      "  (1, 34)\t0.04476614810358452\n",
      "  (1, 44)\t0.04476614810358452\n",
      "  (1, 72)\t0.04476614810358452\n",
      "  (1, 41)\t0.04476614810358452\n",
      "  (1, 66)\t0.04476614810358452\n",
      "  (1, 0)\t0.04476614810358452\n",
      "  (1, 5)\t0.08953229620716904\n",
      "  (1, 42)\t0.04476614810358452\n",
      "  (1, 16)\t0.08953229620716904\n",
      "  (1, 58)\t0.08953229620716904\n",
      "  (1, 24)\t0.04476614810358452\n",
      "  (1, 22)\t0.04476614810358452\n",
      "  (1, 29)\t0.08953229620716904\n",
      "  (1, 8)\t0.04476614810358452\n",
      "  (1, 40)\t0.04476614810358452\n",
      "  (1, 2)\t0.04476614810358452\n",
      "  (1, 19)\t0.04476614810358452\n",
      "  (1, 75)\t0.04476614810358452\n",
      "  (1, 38)\t0.04476614810358452\n",
      "  (1, 39)\t0.04476614810358452\n",
      "  (1, 28)\t0.04476614810358452\n",
      "  (1, 10)\t0.04476614810358452\n",
      "  (1, 63)\t0.04476614810358452\n",
      "  (1, 30)\t0.04476614810358452\n",
      "  (1, 31)\t0.04476614810358452\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "print (cosine_sim(raw1, raw2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 102)\t0.05407494686047413\n",
      "  (0, 10)\t0.37852462802331893\n",
      "  (0, 62)\t0.05407494686047413\n",
      "  (0, 7)\t0.11542431017713858\n",
      "  (0, 109)\t0.11542431017713858\n",
      "  (0, 88)\t0.16222484058142236\n",
      "  (0, 98)\t0.19237385029523096\n",
      "  (0, 107)\t0.10814989372094826\n",
      "  (0, 96)\t0.3462729305314158\n",
      "  (0, 21)\t0.10814989372094826\n",
      "  (0, 59)\t0.10814989372094826\n",
      "  (0, 20)\t0.05407494686047413\n",
      "  (0, 100)\t0.05407494686047413\n",
      "  (0, 95)\t0.0769495401180924\n",
      "  (0, 3)\t0.26932339041332337\n",
      "  (0, 37)\t0.05407494686047413\n",
      "  (0, 49)\t0.05407494686047413\n",
      "  (0, 93)\t0.2162997874418965\n",
      "  (0, 52)\t0.11542431017713858\n",
      "  (0, 29)\t0.0384747700590462\n",
      "  (0, 68)\t0.16222484058142236\n",
      "  (0, 13)\t0.11542431017713858\n",
      "  (0, 5)\t0.16222484058142236\n",
      "  (0, 39)\t0.05407494686047413\n",
      "  (0, 91)\t0.10814989372094826\n",
      "  :\t:\n",
      "  (1, 38)\t0.05648841955182125\n",
      "  (1, 43)\t0.05648841955182125\n",
      "  (1, 60)\t0.05648841955182125\n",
      "  (1, 83)\t0.1129768391036425\n",
      "  (1, 82)\t0.05648841955182125\n",
      "  (1, 97)\t0.05648841955182125\n",
      "  (1, 30)\t0.05648841955182125\n",
      "  (1, 35)\t0.05648841955182125\n",
      "  (1, 8)\t0.05648841955182125\n",
      "  (1, 72)\t0.1129768391036425\n",
      "  (1, 24)\t0.05648841955182125\n",
      "  (1, 105)\t0.16946525865546375\n",
      "  (1, 16)\t0.225953678207285\n",
      "  (1, 80)\t0.05648841955182125\n",
      "  (1, 108)\t0.16946525865546375\n",
      "  (1, 99)\t0.16946525865546375\n",
      "  (1, 48)\t0.16946525865546375\n",
      "  (1, 15)\t0.16946525865546375\n",
      "  (1, 103)\t0.05648841955182125\n",
      "  (1, 73)\t0.1129768391036425\n",
      "  (1, 92)\t0.16946525865546375\n",
      "  (1, 65)\t0.1129768391036425\n",
      "  (1, 33)\t0.1129768391036425\n",
      "  (1, 101)\t0.05648841955182125\n",
      "  (1, 76)\t0.05648841955182125\n",
      "0.18865799710535022\n"
     ]
    }
   ],
   "source": [
    "print (cosine_sim(raw1, raw3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 109)\t0.05503689536336668\n",
      "  (0, 11)\t0.3852582675435668\n",
      "  (0, 65)\t0.03915920434157875\n",
      "  (0, 8)\t0.11747761302473625\n",
      "  (0, 113)\t0.11747761302473625\n",
      "  (0, 93)\t0.16511068609010002\n",
      "  (0, 105)\t0.19579602170789376\n",
      "  (0, 111)\t0.11007379072673336\n",
      "  (0, 103)\t0.35243283907420875\n",
      "  (0, 23)\t0.0783184086831575\n",
      "  (0, 62)\t0.11007379072673336\n",
      "  (0, 22)\t0.05503689536336668\n",
      "  (0, 108)\t0.05503689536336668\n",
      "  (0, 102)\t0.0783184086831575\n",
      "  (0, 3)\t0.27411443039105127\n",
      "  (0, 38)\t0.05503689536336668\n",
      "  (0, 50)\t0.05503689536336668\n",
      "  (0, 99)\t0.156636817366315\n",
      "  (0, 55)\t0.16511068609010002\n",
      "  (0, 32)\t0.03915920434157875\n",
      "  (0, 69)\t0.11747761302473625\n",
      "  (0, 13)\t0.16511068609010002\n",
      "  (0, 6)\t0.16511068609010002\n",
      "  (0, 40)\t0.05503689536336668\n",
      "  (0, 97)\t0.11007379072673336\n",
      "  :\t:\n",
      "  (1, 18)\t0.1829861054400413\n",
      "  (1, 25)\t0.060995368480013766\n",
      "  (1, 86)\t0.12199073696002753\n",
      "  (1, 73)\t0.12199073696002753\n",
      "  (1, 106)\t0.060995368480013766\n",
      "  (1, 5)\t0.12199073696002753\n",
      "  (1, 84)\t0.12199073696002753\n",
      "  (1, 81)\t0.060995368480013766\n",
      "  (1, 64)\t0.060995368480013766\n",
      "  (1, 90)\t0.060995368480013766\n",
      "  (1, 83)\t0.12199073696002753\n",
      "  (1, 48)\t0.12199073696002753\n",
      "  (1, 17)\t0.060995368480013766\n",
      "  (1, 91)\t0.060995368480013766\n",
      "  (1, 112)\t0.060995368480013766\n",
      "  (1, 39)\t0.12199073696002753\n",
      "  (1, 41)\t0.060995368480013766\n",
      "  (1, 85)\t0.060995368480013766\n",
      "  (1, 87)\t0.060995368480013766\n",
      "  (1, 104)\t0.060995368480013766\n",
      "  (1, 33)\t0.060995368480013766\n",
      "  (1, 16)\t0.060995368480013766\n",
      "  (1, 98)\t0.060995368480013766\n",
      "  (1, 107)\t0.060995368480013766\n",
      "  (1, 67)\t0.060995368480013766\n",
      "0.4469576741551711\n"
     ]
    }
   ],
   "source": [
    "print (cosine_sim(raw1, raw4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.06224685265393208\n",
      "  (0, 85)\t0.18674055796179623\n",
      "  (0, 9)\t0.18674055796179623\n",
      "  (0, 46)\t0.26573489001539086\n",
      "  (0, 66)\t0.5602216738853887\n",
      "  (0, 31)\t0.18674055796179623\n",
      "  (0, 8)\t0.1771565933435939\n",
      "  (0, 50)\t0.1771565933435939\n",
      "  (0, 5)\t0.044289148335898476\n",
      "  (0, 22)\t0.044289148335898476\n",
      "  (0, 25)\t0.24898741061572832\n",
      "  (0, 0)\t0.13286744500769543\n",
      "  (0, 48)\t0.06224685265393208\n",
      "  (0, 29)\t0.044289148335898476\n",
      "  (0, 78)\t0.044289148335898476\n",
      "  (0, 76)\t0.13286744500769543\n",
      "  (0, 38)\t0.08857829667179695\n",
      "  (0, 32)\t0.06224685265393208\n",
      "  (0, 43)\t0.06224685265393208\n",
      "  (0, 33)\t0.08857829667179695\n",
      "  (0, 65)\t0.08857829667179695\n",
      "  (0, 23)\t0.044289148335898476\n",
      "  (0, 61)\t0.06224685265393208\n",
      "  (0, 53)\t0.044289148335898476\n",
      "  (0, 20)\t0.06224685265393208\n",
      "  :\t:\n",
      "  (1, 17)\t0.29846841227086496\n",
      "  (1, 49)\t0.29846841227086496\n",
      "  (1, 7)\t0.059693682454172996\n",
      "  (1, 36)\t0.11938736490834599\n",
      "  (1, 41)\t0.23877472981669198\n",
      "  (1, 34)\t0.059693682454172996\n",
      "  (1, 18)\t0.059693682454172996\n",
      "  (1, 64)\t0.11938736490834599\n",
      "  (1, 72)\t0.17908104736251898\n",
      "  (1, 79)\t0.059693682454172996\n",
      "  (1, 3)\t0.11938736490834599\n",
      "  (1, 62)\t0.11938736490834599\n",
      "  (1, 58)\t0.059693682454172996\n",
      "  (1, 44)\t0.059693682454172996\n",
      "  (1, 67)\t0.059693682454172996\n",
      "  (1, 60)\t0.11938736490834599\n",
      "  (1, 54)\t0.059693682454172996\n",
      "  (1, 74)\t0.059693682454172996\n",
      "  (1, 68)\t0.059693682454172996\n",
      "  (1, 30)\t0.059693682454172996\n",
      "  (1, 63)\t0.059693682454172996\n",
      "  (1, 13)\t0.059693682454172996\n",
      "  (1, 70)\t0.059693682454172996\n",
      "  (1, 42)\t0.059693682454172996\n",
      "  (1, 83)\t0.059693682454172996\n",
      "0.26899342776977386\n"
     ]
    }
   ],
   "source": [
    "print (cosine_sim(raw3, raw4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.5015489070943787\n",
      "  (0, 0)\t0.5015489070943787\n",
      "  (0, 1)\t0.7049094889309326\n",
      "  (1, 2)\t0.5015489070943787\n",
      "  (1, 0)\t0.5015489070943787\n",
      "  (1, 3)\t0.7049094889309326\n",
      "0.5031026124151314\n"
     ]
    }
   ],
   "source": [
    "print (cosine_sim('I am happy', 'I am median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.5773502691896258\n",
      "  (0, 0)\t0.5773502691896258\n",
      "  (0, 1)\t0.5773502691896258\n",
      "  (1, 2)\t0.44832087319911734\n",
      "  (1, 0)\t0.44832087319911734\n",
      "  (1, 1)\t0.44832087319911734\n",
      "  (1, 3)\t0.6300993445179441\n",
      "0.7765145304745156\n"
     ]
    }
   ],
   "source": [
    "print (cosine_sim('I am happy', 'I am not happy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t0.5773502691896258\n",
      "  (0, 1)\t0.5773502691896258\n",
      "  (0, 2)\t0.5773502691896258\n",
      "  (1, 3)\t0.3793034928087496\n",
      "  (1, 1)\t0.3793034928087496\n",
      "  (1, 2)\t0.3793034928087496\n",
      "  (1, 0)\t0.5330978245262535\n",
      "  (1, 4)\t0.5330978245262535\n",
      "0.6569729210330906\n"
     ]
    }
   ],
   "source": [
    "print (cosine_sim('I am happy', 'I am a little happy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.7071067811865475\n",
      "  (0, 3)\t0.7071067811865475\n",
      "  (1, 1)\t0.40993714596036396\n",
      "  (1, 3)\t0.40993714596036396\n",
      "  (1, 0)\t0.5761523551647353\n",
      "  (1, 2)\t0.5761523551647353\n",
      "0.5797386715376657\n"
     ]
    }
   ],
   "source": [
    "print (cosine_sim('it rains', 'it does not rain'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n",
    "\n",
    "model = LsiModel(common_corpus, id2word=common_dictionary)\n",
    "vectorized_corpus = model[common_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Word2VecVocab' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-6674be27de94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmodelvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMatrixSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodelvect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Word2VecVocab' object is not iterable"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "              \"The EPS user interface management system\",\n",
    "              \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]\n",
    "sentences = [i.lower().split() for i in documents]\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1, size=10,iter=25)\n",
    "vocab = list(model.vocabulary)\n",
    "modelvect = model[vocab]\n",
    "index = gensim.similarities.MatrixSimilarity(corpus=modelvect, num_features=10)\n",
    "print(index[modelvect[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id2word_wiki' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-c17c2b78bc18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# vectorize the text into bag-of-words and tfidf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mquery_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid2word_wiki\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mquery_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_bow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mquery_lsi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsi_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_tfidf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'id2word_wiki' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"April is the fourth month of the year, and comes between March \\\n",
    "and May. It has 30 days. April begins on the same day of week as July in \\\n",
    "all years and also January in leap years.\"\n",
    "\n",
    "# vectorize the text into bag-of-words and tfidf\n",
    "query_bow = id2word_wiki.doc2bow(tokenize(query))\n",
    "query_tfidf = tfidf_model[query_bow]\n",
    "query_lsi = lsi_model[query_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "702px",
    "left": "0px",
    "right": "1228px",
    "top": "110px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
